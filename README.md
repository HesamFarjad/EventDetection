# EventDetection
Collecting data about frequent occurrences seen in texts, automatically recognising details like what happened and when it did.

First, we install transformers, then import a series of libraries. After that, we mount the drive to connect it. In code block 3, we set the parameters. Then we set the number of epochs to 10 and win_size to 40. In code block 4, it says where to read from and where to save. We create a checkpoint in the code block 4-1 to store the model.
In code block 5, it is assigned to read_json, which obtains it from the same place, and converts it into a Pandas data frame whose output is shown in code block 6, and then printed.
In code block 7, we read the file. The file has two values, one is the label, and the other is the tokens. And we will use these two for modeling. So, we convert the labels. For example, we consider a unique value for each of them and turn it into a dictionary. Here, I will print the lable_dict so that the output is clear. Our lable_dict assigns a range of numbers for each unique label.
In code block 8, the sizes of our sentences are different. The range of changes may be from 3 to 200 or even more. Here, in the first line of the code, we specify a window size in which we put all these labels. (we had defined 40 above). For example, for data that is 10, all of it becomes a list. For illustration, the data has 70 characters, takes the first 40, and puts it in one list, then the other in another. So, two lists created here are nested lists (the length of the first list is 40, and of the second list is 30). Now we have to add a padding here. Furthermore, the length of all these lists is equal; after that, we do the same thing in the second line. If any value is smaller than our win_size, we add a pad for that string value after comparing the lengths of each list.
Now that we have made these lists, there was a sequence of lists. We do the same thing for tokens as we did for labels. We measure the win_size, which should be 40 by 40, and add the padding. Now two lines of code: train lable=np. vstack.
In the typical situation, if the length of our test data set is 20,000, and we split it by 4, it becomes longer and more numerous, resulting in a series of nested lists. We extract these from the nested lists (vstack) and convert them into a matrix, such as an N by 40 matrices, so we may print the value of n (using the len train_lbl) command. We precisely carry out all actions in code block 9 to evaluate the dataset. After taking the label test, we repeat the process for the token test.
In code block 10, we use np. vectorize command. Vectorize applies a connection to the NumPy dataset.
Since we have the output of the label test and the token test, and train_lbl and train_token is the output of the NumPy array, we use vectorizing to apply the function on it.
The first function applied in block 10 is to consider the label dictionary as an output. For example, if so far, our output in the label dictionary, was a pad, instead of that, the output number set on 11, the value of the label dictionary was B action, instead of that, put the number 5, and instead of B change, put the number 3, and so on. At last, convert them to value label dic.
In code block 11, we apply tokens and use the lower to convert the letters to lowercase.
In code block 12, using auto tokenizer, we defined a tokenizer that we apply tokenizer. convert_tokens_to_ids(x).

If so far, for example, there was a word (i) (it was a token), turns it into an ID, and each of these IDs is unique. For instance, the unique ID of (i) becomes 1236.
In code block 13, we define the dataset and data loader together for the train data. So, it is train_dataloader defined in the first line. The tensor dataset is for converting a tensor into a dataset object. Its inputs are torch. tensor (train_tkn) and (train_lbl), which batch_size equals 128; for the test dataset, I only use tensorDataset, which will give us a dataset object.
In code block 14, we define a custom model class. We used an embed layer, then the next layer is LSTM, and the output of the embed is 32, and the input of LSTM is the same as 32 (which is an output of the embed). The output of the LSTM is a 12, and we considered the LSTM to be two-layer, and we defined a self. dropout to prevent over feet, and make the model predict better. Then we apply embed on it; then, assign its output into RNN (LSTM), and apply dropout on it. Furthermore, add a SoftMax layer. To be able to use the value of the output loss function in the model, we use the permute that applies the changes to it.
In code block 15, we apply the model, which becomes the costume model of the model, and we define a cross-entropy in the criterion. We put an ignore index because the ones with index 11, which were our padding, are not inserted into the model, and loss function, to make the need of optimization. Then we set the optimizer to Adam and set the learning rate to 10 to the power of minus three.

The value of the epochs on the range of epochs reads the value of the input. The “pred” becomes the output of the inputs when the model is applied to it. So, the input model is pred. And loss can also be (pred, labels) criterion. We backward the loss output. Now the pred that we have is a vector of 12 which is in one-hot form, so we take an argmax on this one-hot vector to convert it into a number.
Then we reshape it to become linear (not two-dimensional), So, we do the same for the label. Since we don't want it to affect the accuracy, we are printing, we should not consider every place where r_lbl is equal to 11. Only consider those places other than 11 where those paddings are on the output accuracy, f1_score does not affect. And for prediction in the same way. (Do not consider those whose labels are equal to 11).
Then we calculate the accuracy_score. which is real label, and real prediction. Then F1_score is the same. And its average value should be weighted, and print the values of step, loss, accuracy, and F1_Score for each epoch, which is visible in the outputs.
In code block 16 We run these and we want to test them as well. First, we define t_pred and t_lbl as an empty array. For each of our data, it can be test_dataset. For example, the indexes on Len of test_dataset. Our Len of test_dataset was two thousand, so our index can be from zero to 1999. Then get_item from test_dataset for index. (for example, in test_dataset, give us the output of the first row) whose outputs will be assigned on test_input, and test_lbl.

Now we put test_input inside the model, but before that, we do an un-squeeze on it, because when we put loader in the model, the value of our vector becomes two-dimensional. (for example, a vector 128 * 40) but as there is no batch_size here, and we get the output from the dataset, then the output is a vector of 40. Here we say do an un-squeeze it to be a 1 x 40 vector. We put this into the model and get its output, and the output is a vector, 1 x 12 x 40, then we take its argmax to make those 12.
We always squeeze a 1 x 40 vector to make it a 40-x vector. Again, into these 40 vectors, consider the places where those labels are not 11. We get test_lbl again, which can be real_test label, real test pred for where our test label is not 11.
Now we have np. hstack test pred. We had an empty total pred, we had an rt_pred, and we convert it to NumPy array format. We add each re_pred output series to t_pred, and add rt_lbl output to t_lable. And finally, after 2000 iterations, the value of t_pred, and t_lbl is formed.
In code block 17, we defined the accuracy test, recall test, and f1 test. In code block 18, we want it to print all this for us. In code block 19, we define the confusion matrix plot, and finally, it plots the confusion matrix for us, the output we have shown.
